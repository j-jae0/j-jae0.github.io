---
title:  "[ML] 경사하강법(Gradient Descent)이란?"
layout: single

categories: "ML"
tags: ["경사하강법", "Loss"]

toc: true
toc_sticky: true
toc_label : "목차"
toc_icon: "bars"
---

<small>함수 최적화 방법 중 하나인 경사하강법(Gradient Descent)에 대해 알아본다.</small>

***

> Gradient 를 이용해서 함숫값을 줄인다고 해서 **Gradient Descent** 라는 표현을 사용한다. 경사하강법은 1차 미분계수를 이용해 함수의 최소값을 찾아가는 iterative한 방법이다.

<br>

# <span class="half_HL">✔️ 경사하강법</span>
**경사하강법(Gradient Descent)** 은 기본적인 함수 최적화(optimization) 방법 중 하나이다. Steepest Descent 방법이라고도 불린다. 여기서 최적화란 함수의 최댓값 또는 최솟값을 찾는 것을 말한다.

<br>

<div style="text-align : center;">
<img src="https://ifh.cc/g/6746FS.png" width="400">
</div>

<br>

<span class="HL">머신러닝에서는 Loss를 최소화해야 가장 좋은 퍼포먼스를 보여줄 수 있기 때문에 Loss를 minimize 하는 parameter를 찾는 것이 핵심이다.</span>

<br>

<div style="text-align : center;">
<img src="https://ifh.cc/g/C31nh8.png">
</div>

<br>

현재(```Wcurrent```) 위치에서 Loss가 가장 작은 지점(```Wbest```)으로 찾아 나아가는 과정이다. Gradient를 구하고 Gradient 방향으로 해를 계속 업데이트함으로써 최적의 해를 찾는다. 

Loss 함수가 이차함수라면 미분값이 0인 지점을 기준으로 최소값을 찾을 수 있지만 아래 그래프와 같은 함수는 매우 복잡하기 때문에 오차가 최소인 지점을 찾기가 어렵다. 

<div style="text-align : center;">
<img src="https://ifh.cc/g/88VGPA.jpg">
</div>
<center><small>3차원 그래프(왼쪽), 3차원 정보를 2차원으로 표현한 등고선(오른쪽)</small></center>

<br>

3차원 그래프(왼쪽)를 그리게 되면 안보이는 부분이 많다는 단점이 있다. 또한 위 그림과 같이 미분값이 0인 지점이 여러개 있기 때문에 전역적인 최적해(Global Optimum)를 구할 수 있다는 보장이 없다.

이때 3차원 그래프의 단점을 보완하기 위해 3차원 정보를 2차원인 등고선(오른쪽)으로 표현한다. 각 영역마다 Loss가 정의되는데 등고선에 그려진 붉은 선은 가장 작은 Loss 를 찾는 과정이다.


## (1) 경사하강법의 유도

등산의 경우 산 꼭대기에서 최저점으로 하산하려면 비탈길을 타고 계속 내려오면 된다. 이와 마찬가지로 Loss 함수도 경사(기울기)가 아래로 향하는 방향으로 계속 이동하다보면 Loss가 최저점인 
지점에 도달하게 된다.

<br>

<div style="text-align : center;">
<img src="https://ifh.cc/g/FSBD8d.jpg" width="400">
</div>

<br>

이때 기울기는 **그래디언트(gradient, ∇)** 라고 한다. 만약 그래디언트가 (-)라면 하산은 (+) 방향으로 이동한다. 반대로 그래디언트가 (+)라면 하산은 (-) 방향으로 이동한다.

**이동거리** 는 <u>그래디언트와 비례</u>하므로 그래디언트가 크면 이동거리가 크게 증가하고 그래디언트가 작으면 이동거리는 작게 증가한다. 


## (2) 하이퍼 파라미터 설정

경사하강법에 있어서 학습률을 조절하여 **학습 속도(Learning rate)** 를 증가시킬 수 있다. 그러나 <u>step size가 너무 커버리면 함수값이 커지는 방향으로 최적화가 이루어진다.</u>

<br>

<div style="text-align : center;">
<img src="https://ifh.cc/g/Y16pky.png">
</div>

<br>

<span class="HL">대용량 데이터를 대상으로 할 때 Step size를 작게(학습속도 느림) 설정한다면 시간이 매우 오래걸릴 수 있다. 또한 Minimun 지점에 수렴하지 못할 수도 있으므로 적절한 Step size를 설정해줘야 한다.</span>

## (2) 문제점
경사하강법의 경우 아래 그래프와 같이 local minimum에 빠지는 경우가 생길 수 있다. 

최솟값은 global minimum이지만 local minimum을 손실(Loss)의 최솟값으로 정의한다면 모델 성능이 나빠질 수 있다. 그러나 MSE의 경우 볼록 함수(convex function)로 local minimun에 빠질 가능성이 없으며, 고차원의 공간에서는 local minimum에 빠질 가능성이 현저히 낮으므로 큰 문제가 되지 않는다.


<br>

<div style="text-align : center;">
<img src="https://ifh.cc/g/SHbnL1.png" width="400">
</div>

<br>

# <span class="half_HL">✔️ Reference</span>
- [⛰️ 이미지 출처](https://supernaturalpowers.fandom.com/ko/wiki/%EC%82%B0_%EC%A1%B0%EC%9E%91)
- [K-MOOC, 김영훈 교수님 - 실습으로 배우는 머신러닝](http://www.kmooc.kr/courses/course-v1:SSUk+SSMOOC20K+2022_T2/about)
- [Github 블로그 by angeloyeo - 경사하강법(gradient descent)](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)
- [로봇스토리 컴퓨터반 게시판 - 머신러닝(Machine Learning) : 경사하강법(Gradient Descent)](https://www.robotstory.co.kr/raspberry/?mode=view&board_pid=63)
- [티스토리 by yooj_lee - [ML/DL] 최적화(Optimization), 경사하강법 (Gradient Descent Algorithms)](https://daebaq27.tistory.com/35)

<br>

👩🏻‍💻개인 공부 기록용 블로그입니다
<br>오류나 틀린 부분이 있을 경우 댓글 혹은 메일로 따끔하게 지적해주시면 감사하겠습니다.
{: .notice}