---
title:  "[ML] 배깅(Bagging)과 부스팅(Boosting) 이해하기"
layout: single

categories: "ML"
tags: ["배깅", "부스팅", "편향", "분산"]

toc: true
toc_sticky: true
toc_label : "목차"
toc_icon: "bars"
---

<small>모델의 예측 성능을 향상시키기 위해 사용되는 배깅 기법과, 부스팅 기법에 대해 알아본다.</small>

***

> **배깅(Bagging)** 과 **부스팅(Boosting)** 은 <u>앙상블 기법</u>이라는 점에서 비슷합니다. weak learner를 결합하여 single learner보다 더 나은 성능을 얻는 strong learner를 만듭니다.<br>
<br><u>Bagging 및 Boosting과 같은 기술을 사용하면 분산을 줄이고 모델의 견고성을 높일 수 있습니다.</u>

<br>

# <span class="half_HL">✔️ 배깅(Bagging)</span>
Bagging은 bootstrap aggregating의 약자로, 부트스트랩(bootstrap)을 통해 조금씩 다른 훈련 데이터에 대해 훈련된 기초 분류기(base learner)들을 결합(aggregating) 시키는 방법입니다.


## (1) 배깅(Bagging)을 통해 랜덤포레스트 훈련 과정
- <u>부트스트랩(Bootstrap)</u> 을 통해 T개의 훈련 데이터셋을 생성한다.
- T개의 기초 분류기(트리)들을 훈련한다.
- 기초 분류기(트리)들을 하나의 분류기(랜덤 포레스트)로 결합한다
  - 결합 방법 : 평균 또는 과반수 투표 방식 이용

<br>

<div style="text-align : center;">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Randomforests_ensemble.gif/440px-Randomforests_ensemble.gif" width="500">
</div>
<center><small>Fig 1. 배깅을 통한 랜덤포레스트 훈련 과정</small></center>

<br>

## (2) 부트스트랩(bootstrap) 이란?
주어진 훈련 데이터에서 <u>중복을 허용하여 랜덤하게 추출하여</u> 동일한 사이즈의 새로운 데이터셋을 여러 개 만드는 과정이다.

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1400/1*YYom-NKDaZ-B7RB_891DgQ.png" width="500">
</div>
<center><small>Fig 2. 부트스트래핑(Bootstrapping) 과정</small></center>

<br>

부트스트랩을 사용하면 데이터셋의 편향과 분산을 더 잘 이해할 수 있습니다. 랜덤하게 추출된 여러 데이터셋은 기존 데이터와는 다른 왜곡된 데이터 분포를 갖게 됩니다. <u>기존 데이터셋만 사용하여 학습했을 때는 하나의 특정한 노이즈(epsilon)에만 종속적인 모델이 만들어질 위험이 있습니다.</u>

하지만 **Bagging** 은 조금씩 분포가 다른 데이터셋을 만들고(bootstrap), 그 데이터셋을 기반으로 반복 학습 및 모델 결합을 통해 이러한 위험을 방지하는 효과를 가집니다.


## (3) 편향-분산 트레이드오프(bias-variance tradeoff) 란?
>**트레이드오프(Tradeoff)의 목표**는 결정 경계에 대한 적절한 복잡성을 찾는 것입니다.<br>
<br>**1. High complexity** : 과거를 기억하고 미래를 위해선 일반화하지 않을 수 있음(High variance problem)<br>
**2. Low complexity** : 매우 단순한 의사 결정 경계 때문에 과거로부터 충분히 배울 수 없었고 다시 좋은 예측을 하지 못할 수 있음 (High bias problem)<br><br>
**편향** 은 예측값(predict)이 실제 정답값과 얼마나 멀리 떨어져 있는지로 측정할 수 있습니다. **분산** 은 예측값들끼리의 차이로 측정할 수 있습니다.

<div style="text-align : center;">
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FvhlZ1%2FbtqyfOOjKul%2FhyJrHX8TmrBY8uJvcQVoo0%2Fimg.png" width="300">
</div>
<center><small>Fig 3. 편향(Bias)과 분산(Variance)의 관계</small></center>

<br>

**편향(Bias)** 은 지나치게 단순한 모델로 인한 error 입니다. 편향이 크면(정답값에서 멀어짐) <u>과소 적합(Under-Fitting)을 야기하고</u> 그 모델의 중요한 요소를 놓치고 있다는 뜻입니다.<br>
**분산(Variance)** 은 지나치게 복잡한 모델로 인한 error 입니다. 분산이 크면(예측값들이 흩어짐) <u>과대 적합(Over-Fitting)을 야기하고</u> train dataset에 지나치게 적합시켜 일반화가 되지 않은 모델입니다.

<br>

<div style="text-align : center;">
<img src="https://i.stack.imgur.com/6Y87l.png" width="500">
</div>
<center><small>Fig 4. 편향(Bias)과 분산(Variance)에 따른 모델 적합도</small></center>

<br>

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1400/1*kISLC1Udq0m6g5kwHhMuJg@2x.webp" width="350">
</div>
<center><small>Fig 5. 편향(Bias) 및 분산(Variance)과 Model Complexity 의 관계</small></center>

<br>

모델이 복잡해질 수록 편향은 작아지고 분산은 커집니다.(over-fitting) 모델이 단순해질수록 편향은 커지고 분산은 작아집니다(under-fitting). 위 그래프를 보면 무조건 편향만 줄일 수도 무조건 분산을 줄일 수도 없습니다.<br>

<u>🚨 오류(error)를 최소화하려면 편향과 분산의 합이 최소가 되는 적당한 지점을 찾아야 합니다.</u>

<br>

# <span class="half_HL">✔️ 부스팅(Boosting)</span>
부스팅은 말 그대로 성능을 부스트(boost, 끌어올린다)하려는 목적을 갖습니다. 단일 모델의 성능이 부족하고, 또는 트레이닝 데이터의 양과 질이 부족할 때 여러 모델들을 활용해서 점점 성능을 높일 수 있습니다. 특히 트레이닝 데이터에도 예측 성능이 잘 나오지 않을 때(Under-Fitting) 사용합니다.

<br>

<div style="text-align : center;">
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FkCejr%2FbtqyghvqEZB%2F9o3rKTEsuSIDHEfelYFJlk%2Fimg.png" width="500">
</div>
<center><small>Fig 6. 부스팅의 가충치 부여 과정</small></center>

<br>

Boosting은 가중 평균을 활용하여 weak learner를 strong learner로 만드는 알고리즘 그룹이며, 각 모델이 독립적으로 실행된 다음 어떤 모델도 선호하지 않고 마지막에 출력을 집계하는 배깅(Bagging)과는 다릅니다. 부스팅은 모두 ```"TeamWork"``` 에 관한 것입니다. <u>실행되는 각 모델은 다음 모델이 집중할 기능을 나타냅니다.</u>

<br>

<div style="text-align : center;">
<img src="https://quantdare.com/wp-content/uploads/2016/04/bb3.png" width="500">
</div>
<center><small>Fig 7. Bagging과 Boosting의 모델 선정 과정</small></center>

<br>

예를 들어, 수학 문제를 푸는데 10문제 중 7개를 맞췄다면 **부스팅 방식**은 틀린 3문제에 가중치를 부여해서 이전에 틀렸던 3문제를 잘 맞춘 모델을 최종 모델로 선정합니다. <br>

즉, Boosting도 Bagging과 동일하게 복원 랜덤 샘플링을 하지만, **가중치**를 부여한다는 차이점이 있습니다. 또한 **Fig 5** 를 보면 알 수 있듯이 Bagging은 여러 데이터셋에 대해 병렬로 학습하는 반면 Boosting은 순차적으로 학습시킵니다. 그리고 Boosting 방식은 한 모델에 대해 학습이 끝나면 결과에 따라 <u>가중치가 재분배되어 다음 모델을 학습시키는데 반영합니다.</u>

오답에 대해 높은 가중치를 부여하고, 정답에 대해 낮은 가중치를 부여하기 때문에 오답에 더욱 집중할 수 있어 정확도가 높게 나타납니다. 하지만 그만큼 **Outlier에 취약**하다는 단점을 가지고 있습니다.

> 🤔 **Boost 알고리즘 모델**
<br>AdaBoost, XGBoost, GradientBoost 등의 다양한 알고리즘이 있는데 그 중에서도 **XGBoost** 모델은 강력한 성능을 보여줘 최근 대부분의 Kaggle 대회 우승 알고리즘으로 사용되고 있습니다.

<br>

# <span class="half_HL">✔️ 배깅과 부스팅 비교하기</span>
- **Parallel** vs **Sequential**
  - ```Bagging```은 동시에 예측한 후 결과물을 결합함
  - ```Boosting```은 하나의 모델이 한번 예측하고 나면 그 결과를 바탕으로 다음 모델이 작동함
    - 이러한 학습 과정의 차이는 자연스럽게 학습 시간에서 차이가 발생함

- **Overfit** vs **Underfit**
  - ```Bagging```은 단일 모델이 너무 학습 데이터에 fit되는 상황에서 사용됨
  - ```Boosting```은 단일 모델의 성능이 약할 때 사용될 수 있음

- **Randomly(Bootstrap)** vs **Weight**
  - ```Bagging```은 데이터나 feature(변수)를 샘플링할 때 무작위로 뽑아냄
    - 데이터는 복원추출, feature는 한번 추출할 때는 비복원으로 추출
  - ```Boosting```은 앞선 모델의 결과를 반영해 더 좋은 성능을 보여야하기 때문에 예측이 틀린 케이스를 중심으로 샘플링을해서 추가 학습을 진행함(틀린 문제에 가중치를 두고 다음 모델에 힌트를 줌)

<br>

# <span class="half_HL">✔️ 배깅과 부스팅 중 어느것이 좋을까?</span>
배깅과 부스팅을 선택하는 기준은 데이터, 상황, 시뮬레이션 등에 따라 달라집니다. 배깅 및 부스팅은 서로 다른 모델의 여러 추정치를 결합하여 단일 추정치의 분산을 줄여줍니다(단일 모델에 대한 문제점 해결) 

## (1) 단일 모델의 성능이 낮은 경우
개별 결정 트리 모델의 성능이 낮은 상황에서 Bagging을 사용한다면 더 나은 편향(bias)을 거의 얻지 못할 것입니다.
Boosting은 단일 모델의 이점을 최적화하고 함정을 줄이기 때문에 오류가 적은 결합 모델을 생성할 수 있습니다.

## (2) 단일 모델에 과적합된 경우
부스팅은 과적합을 피하는데 도움이 되지 않기 때문에 Bagging이 최선의 선택입니다.

<br>

# <span class="half_HL">➕ 복원추출과 비복원추출이란?</span>
>**복원 추출** : 한번 뽑은 표본을 모집단에 다시 넣고 다른 표본을 추출<br>
**비복원 추출** : 한번 뽑은 표본을 모집단에 다시 넣지 않고 다른 표본을 추출

<br>

<div style="text-align : center;">
<img src="https://ifh.cc/g/dmh4bW.jpg" width="500">
</div>
<center><small>Fig 8. 복원추출과 비복원추출</small></center>

<br>

## (1) 복원 추출(Sampling with replacement: SWR)
처음 모집단에서 추출된 표본을 되돌려 넣고 다음 표본을 추출하는 방법입니다. 그렇기 때문에 동일한 표본이 중복해서 선택될 수 있습니다. 이 방식은 표본을 뽑은 후 모집단을 다시 복원시키기 때문에 표본공간은 독립적으로 변화가 없습니다.

<u>이 추출방법은 배깅의 bootstrap에서 사용됩니다.</u>

## (2) 비복원 추출(Sampling without replacement: SWOR)
처음 모집단에서 추출된 표본을 되돌려 넣지 않고 다음 표본을 추출하는 방법입니다. 따라서 표본을 하나 하나 추출하는 행위는 표본공간을 바꾸는 종속사건이 됩니다. 즉, 표본을 추출하면 다음 표본들의 추출 확률에 영향을 미치게 됩니다.


<br>

# <span class="half_HL">✔️ Reference</span>
- [멋쟁이사자처럼 AI SCHOOL] 박조은 강사님 강의자료
- [위키백과, 우리 모두의 백과사전 - 랜덤 포레스트](https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8)
- [WIKIPEDIA, The Free Encyclopedia - Boosting (machine learning)](https://en.wikipedia.org/wiki/Boosting_(machine_learning))
- [티스토리 by 귀퉁이 서재, 머신러닝 - 11. 앙상블 학습 (Ensemble Learning): 배깅(Bagging)과 부스팅(Boosting)](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning-%EB%B0%B0%EA%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting)
- [티스토리 by 귀퉁이 서재, 머신러닝 - 12. 편향(Bias)과 분산(Variance) Trade-off](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-12-%ED%8E%B8%ED%96%A5Bias%EC%99%80-%EB%B6%84%EC%82%B0Variance-Trade-off)
- [티스토리 by sungkee - [머신러닝] 앙상블 학습 - 2) Bagging](https://sungkee-book.tistory.com/9)
- [벨로그 by jonas-jun, [ML] 앙상블이란? 배깅과 부스팅 비교](https://velog.io/@jonas-jun/ML-%EC%95%99%EC%83%81%EB%B8%94)
- [네이버블로그 by 낙수, 표본추출 방법! 층화추출, 군집추출, 복원추출, 비복원추출](https://blog.naver.com/PostView.nhn?blogId=angryking&logNo=222363296910&parentCategoryNo=&categoryNo=26&viewDate=&isShowPopularPosts=true&from=search)
- [Github 블로그 by swallow, Bagging과 Boosting 그리고 Stacking](https://swalloow.github.io/bagging-boosting/)
- [Medium by Joseph Rocca, Ensemble methods: bagging, boosting and stacking](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)
- [Medium by Jinde Shubham, Ensemble Learning — Bagging and Boosting](https://becominghuman.ai/ensemble-learning-bagging-and-boosting-d20f38be9b1e)
- [StackoverFlow - How to quantify bias and variance given train data samples](https://stackoverflow.com/questions/58847171/how-to-quantify-bias-and-variance-given-train-data-samples)

<br>

👩🏻‍💻개인 공부 기록용 블로그입니다
<br>오류나 틀린 부분이 있을 경우 댓글 혹은 메일로 따끔하게 지적해주시면 감사하겠습니다.
{: .notice}