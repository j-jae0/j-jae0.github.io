---
title:  "[ML] 의사결정나무(Decision Tree) - 지니계수, 엔트로피"
layout: single

categories: "ML"
tags: ["지니계수", "엔트로피"]

toc: true
toc_sticky: true
toc_label : "목차"
toc_icon: "bars"

use_math: true
---

***

> **의사 결정 나무(Decision Tree)**는 주어진 입력값들의 조합에 대한 의사결정규칙(rule)에 따라 출력값을 예측하는 모형<br>
<br><u>의사결정나무 모델은 불순도가 낮아지는 방향</u>으로 학습을 하는데, 이때 지니계수와 엔트로피 가 사용됩니다. 

<br>

# 지니계수와 엔트로피
- CART 알고리즘에서 사용되는 것으로, 트리를 그릴 때 추가하여 값을 볼 수 있습니다.
- 사용 목적 
  - 분류를 할 때 True, False 로 완전히 나눠지지 않는데, 이때 값이 얼마나 섞여있는지를 수치로 나타내기 위해 사용합니다.
  - 0에 가까울수록 다른 값이 섞여있지 않은 상태입니다.
  - 분류의 분할에 대한 품질을 평가할 때 사용합니다.

<div style="text-align : center;">
<img src="https://tensorflowkorea.files.wordpress.com/2017/06/2-24.png?w=1250" width="400" >
</div>
<center><small>Fig 1. 깊이 1인 결정트리에 대한 결정 경계</small></center>

<br>

<div style="text-align : center;">
<img src="https://tensorflowkorea.files.wordpress.com/2017/06/2-25.png?w=1250" width="400" >
</div>
<center><small>Fig 2. 깊이 2인 결정트리에 대한 결정 경계</small></center>

<br>

<div style="text-align : center;">
<img src="https://tensorflowkorea.files.wordpress.com/2017/06/2-26.png?w=1250" width="400" >
</div>
<center><small>Fig 3. 깊이 9인 결정트리에 대한 결정 경계</small></center>

<br>

# 지니계수(Gini Index)
- 집합에 이질적인 것이 얼마나 섞여있는지를 측정하느 지표입니다.
- 어떤 집합에서 한 항목을 뽑아 무작위로 라벨을 추정할 때, 틀릴 확률을 말합니다.
- 집합에 있는 항목이 모두 같다면, 지니 불순도는 최솟값(0)을 갖게 되며 이 집합은 완전히 순수하다고 말할 수 있습니다.

## 예시로 이해하기
- 주머니에 빨간색 구슬 7개, 초록색 구슬 3개가 들어있다고 가졍하여 봅시다.
  - 그럼 아래의 지니 불순도 계산식에 의해 0.42 라는 값이 나옵니다.
  - 즉, 주머니 속 초록색 구슬(불순물)이 42% 차지하고 있다는 뜻입니다.

<br>

- 식당에 갔는데 짜장면, 짬뽕 2 가지 메뉴만 존재한다고 가정하여 봅시다.
  - 선택지 경우는 2가지가 됩니다.
  - 그럼 아래 지니 불순도 계산식에 의해 0.5 라는 값이 나옵니다.
  - 반반인 경우는 최악의 경우를 말합니다.

<br>

<div style="text-align : center;">
<img src="https://ifh.cc/g/CqbT2M.png" width=600>
</div>
<center><small>지니 불순도 계산식</small></center>

<br>

# 정보 엔트로피(Entropy)
- 확률분포가 가지는 정보의 확신도 혹은 정보량을 수치로 표현한 것입니다.
- 확률분포에서 특정 값이 나올 확률이 높아지고 나머지 값의 확률이 낮아지면 엔트로피는 작은 값을 가집니다.
- 엔트로피가 높다는 것은 불순도가 높다는 뜻이고, 엔트로피가 낮다는 것은 불순도가 낮다는 뜻입니다.
- 즉, 엔트로피가 0이면 불순도는 최소이고 한 범주 안에 하나의 데이터만 있다는 뜻입니다.

## 예시로 이해하기
- Fig 1 의 결정 경계로 엔트로피 값 구해보기

  - 파란색 영역
    - 파란색 동그라미(48개), 빨간색 세모(18개) 를 포함합니다.
    - 아래의 엔트로피 계산식에 의해 ```0.845``` 라는 값이 나옵니다.

  - 빨간색 영역
    - 파란색 동그라미(2개), 빨산색 세모(32개) 를 포함합니다.
    - 아래의 엔트로피 계산식에 의해 ```0.323``` 라는 값이 나옵니다.

<br>

<!-- $$
H(x) = - \sum_{i=1}^{m}{f_ilog_{2}f_2}
$$
<center><small>섀넌 엔트로피 계산식</small></center> -->

<div style="text-align : center;">
<img src="https://ifh.cc/g/G0yaAR.png" width=200>
</div>
<center><small>섀넌 엔트로피 계산식</small></center>

<br>

# Reference
- 📷 이미지 출처 [[텐서플로우 블로그 (Tensor ≈ Blog)] - 2.3.5 결정 트리](https://tensorflow.blog/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2-3-5-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC/)
- [[위키백과, 우리 모두의 백과사전] - 결정 트리 학습법](https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95)
- [[위키백과, 우리 모두의 백과사전] - 정보 엔트로피](https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC)
- [[티스토리 by 볼록티] - 지니불순도(Gini impurity)](https://data-science-hi.tistory.com/59)
- [[데이터 사이언스 스쿨] - 10.1 엔트로피](https://datascienceschool.net/02%20mathematics/10.01%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.html)
- [[티스토리 by 후이] - [Machine learning] 의사결정나무 - 지니계수(gini-index), Cross entropy, 정보이득 (information gain), ID3, C4.5, CART](https://huidea.tistory.com/273)
- [[데이터 과학] - 10 장 의사결정나무](http://bigdata.dongguk.ac.kr/lectures/datascience/_book/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4tree-model.html)

<br>

**개인 공부 기록용 블로그입니다**
<br>오류나 틀린 부분이 있을 경우 댓글 혹은 메일로 따끔하게 지적해주시면 감사하겠습니다 :)
{: .notice}