---
title:  "[DL] 옵티마이저(Optimizer)의 필요성 및 종류"
layout: single

categories: "DL"
# tags: ["활성화함수", "Sigmoid", "Tanh", "ReLU", "Leaky ReLU"]

toc: true
toc_sticky: true
toc_label : "목차"
toc_icon: "bars"
published: false
---

<small>Deep Learning에서 고려해야하는 Optimizer에 대해 알아본다.</small>

***

# <span class="half_HL">✔️ Optimizer 란?</span>
**optimizer**는 <u>오차 함수(손실 함수)를 최소화하거나 생산 효율을 최대화</u>하기 위해 사용되는 알고리즘 또는 방법이다. 
- 가중치 및 편향과 같은 모델의 학습 가능한 매개변수에 따라 달라지는 수학 함수
- 손실을 줄이기 위해 신경망의 가중치와 학습 속도를 변경하는 방법을 알도록 도와줌

**Optimizer**는 손실 함수(loss function)을 통해 구한 차이를 사용해 기울기를 구하고 Network의 parameter(W, b)를 학습에 어떻게 반영할 것인지를 결정하는 방법이다. 즉, **loss function의 최솟값을 찾는 것이 최적화(Optimization)이고 이를 수행하는 알고리즘이 최적화 알고리즘(Optimizer)** 이다.<br>
<small>cf. W:가중치(Weight), b:편향(bias)</small>

<br>

# <span class="half_HL">✔️ Optimizer의 필요성</span>
**딥러닝**은 입력층(Input layer)과 출력층(Output layer) 사이에 여러 개의 은닉층(Hidden layer)으로 이루어진 신경망이다.

층이 깊어질 수록 모듈과 함수에 따른 하이퍼파라미터(Hyperparameter)도 비례하여 많아지기에 이 하이퍼파라미터를 결정하여 모델이 정확하게 결과를 뱉어낼 수 있도록 하는 것이 학습의 핵심이다.

그러기 위해 Loss Function을 정의해야하고, **손실함수를 최적화**해야한다.

<br>

# <span class="half_HL">✔️ Optimizer의 종류</span>

<div style="text-align : center;">
<img src="https://ifh.cc/g/Fw89fX.jpg" width="500">
</div>
<center><small>이미지 출처 : https://www.slideshare.net/yongho/ss-79607172?from_action=save</small></center>

<br>

다양한 유형의 옵티마이저와 손실 함수를 최소화하기 위해 정확히 어떻게 작동하는지 알아본다.


## 1. Gradient Descent(GD)


## 2. Stochastic Gradient Descent(SGD)

## 3. Mini-Batch Gradient Descent

## 4. SGD with Momentum

## 5. AdaGrad(Adaptive Gradient Descent)

## 6. RMS-Prop (Root Mean Square Propagation)

## 7. AdaDelta

## 적절한 Optimizer 찾는 방법

# <span class="half_HL">✔️ Reference</span>


